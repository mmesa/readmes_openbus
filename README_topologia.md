# Topologías

Los pasos son los siguientes:

* Descargar el proyecto de topologías que contiene todo el paquete necesario para poder lanzar topologías a storm.
  Pinchar en este [enlace](https://github.com/Produban/openbus/tree/topologias) para ir al repositorio donde se encuentra el proyecto, una vez ahí existe un botón a la derecha para poder descargar el mismo.

* Una vez descargado el proyecto descomprimirlo en una carpeta y dentro de la ruta topologias/src/main/java/com/produban/openbus/topologies es donde hay que crear las dos clases del parseo y de definicion de
la topología para el nuevo origen. La clase parseador es la que utiliza las funciones de Trident que permite procesar cada registro del tópico Kafka y dividirlo en campos. La clase de definición de topologías es la que submitirá cada topología al cluster de Storm. Se podría tomar como ejemplo las clases del origen Radius: RadiusEntityParser.java y
OpenbusRadiusEntityTopology.java para la creación de las nuevas para el nuevo origen.

* Cuando las clases esté implementadas y compiladas hay que generar el archivo .jar con todo el paquete de clases de topología y parseo. Una forma de hacerlo sencillo es con apache maven.



* Es necesario crear un archivo .properties por cada nuevo origen con las siguientes propiedades:

--Configuración acceso HDFS
**HDFS_URL**=Url y puerto donde está el HDFS  
**HDFS_USER**=Usuario para escribir en HDFS  
**HDFS_OUTPUT_DIR**=Directorio donde se escribirá los datos parseados  
**HDFS_OUTPUT_FILENAME**=Prefijo del nombre del fichero de salida  
**HDFS_OUTPUT_FILE_EXTENSION**=Extensión del fichero de salida  
**INPUT_ORIGIN**=Se usa para especificar de dónde serán leídos los datos, las opciones posible son kafka y disco  
--Configuración de KAFKA   
**KAFKA_ZOOKEEPER_LIST**=Lista de url:puerto donde esta corriendo el Zookeeper  
**KAFAKA_BROKER_ID**=Id del broker Kafka  
**KAFKA_TOPIC**=Nombre del tópico Kafka del origen  
**KAFKA_FROM_BEGINNING**=Valor true o false para especificar que Kafka lea desde el principio o no  
--Configuración STORM  
**STORM_CLUSTER**=Indica si la topología sera desplegada en el cluster local o servidor, los valores posibles son local o server  
**STORM_BATCH_MILLIS**=Tiempo batch en milisegundos  
**STORM_NUM_WORKERS**=Número de workers  
**STORM_MAX_SPOUT_PENDING**=Número máximo de spouts pendientes  
**STORM_TOPOLOGY_NAME**=Nombre de la topología  
--Propiedades de la rotación  
**SIZE_ROTATION_UNIT**=Unidades de medida, los valores posibles son KB/MB/GB/TB  
**SIZE_ROTATION_VALUE**=Valor de la medida  
**TIME_ROTATION_UNIT**=Unidad de tiempo, los valores posibles son SECOND/MINUTE/HOUR/DAY  
**TIME_ROTATION_VALUE**=Valor de tiempo  
--Propiedades de la sincronización/escritura a HDFS  
**SYNC_MILLIS_PERIOD**=Periodo en milisegundos en el que los datos parseados serán escritos en disco   
--Configuración ElasticSearch  
**ELASTICSEARCH_HOST**=Url donde está instalado Elasticsearch  
**ELASTICSEARCH_PORT**=Puerto donde está instalado Elasticsearch  
**ELASTICSEARCH_NAME**=Nombre de elasticsearch   
**ELASTICSEARCH_CACHE_SEARCH**=Permite cachear las búsquedas en elasticsearch, los valores posibles son true o false  


* Para ejecutar la topología es necesario desplegar el archivo .jar generado en el entorno de Storm en el directorio de instalación del servidor donde se encuentre storm de esta manera:

`./bin/storm jar <JAR_FILE> <Main_Topology_class> <properties_file>`. Donde <JAR_FILE> es el archivo .jar generado anteriormente con todas las clases de topología y parseo. <Main_Topology_class> es la clase de definición de topología que se quiere desplegar. Y <properties_file> el archivo .properties creado con las propiedades para el nuevo origen.

Por ejemplo

`./bin/storm jar topologies-0.0.1-SNAPSHOT.jar com.produban.openbus.topologies.OpenbusRadiusEntityTopology radius.properties`


[Configuración de la topología de los otros orígenes]
(https://github.com/mmesa/readmes_openbus/blob/master/README_topologia_conf.md)
